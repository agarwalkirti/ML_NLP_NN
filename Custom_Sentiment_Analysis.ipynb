{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "awPXefiYqQsF",
    "outputId": "be602c63-f5c0-4276-b33c-ad0808258677"
   },
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "import pandas as pd\n",
    "path = 'C:/Users/akirt/smsspancollection/SMSSpamCollection.txt' \n",
    "df = pd.read_csv(path, sep='\\t', names=[\"label\", \"message\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oANCBt_0q22q",
    "outputId": "ec856094-7238-482f-e533-72a7413fa0cc"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsvEwG98qq2b"
   },
   "outputs": [],
   "source": [
    "X=list(df['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bJRCocDrFvE"
   },
   "outputs": [],
   "source": [
    "y=list(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gz4pK1t3s5D5",
    "outputId": "6bd9e757-0067-409d-e68a-fe7545344e38"
   },
   "outputs": [],
   "source": [
    "#y #see they are labels in letters we have to convert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZpBn3MtssZL"
   },
   "outputs": [],
   "source": [
    "y=list((pd.get_dummies(y,drop_first=True)['spam']).astype(int)) #get_dummies does one hot encoding so using list() converts it to integer labels \n",
    "#else use argmax for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLFDWda0rIKw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "al2HoNEjrMje",
    "outputId": "0de86426-dcdc-473b-8375-7be7fd14ef84"
   },
   "outputs": [],
   "source": [
    "#Ensure labels are integers, not one-hot encoded\n",
    "#Hugging Face will automatically pick CrossEntropyLoss (correct for classification) if labels are shaped (batch_size,).\n",
    "#In NumPy, np.argmax(array, axis) returns the index of the maximum value along a given axis. \n",
    "import numpy as np\n",
    "#y_train = np.argmax(y_train.values, axis=1) #So it converts the one-hot vectors into integer class IDs.\n",
    "#y_test  = np.argmax(y_test.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqOBGiGErZgj",
    "outputId": "99d8de1d-55f6-43d9-a9be-b69cde0a3bad"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "7601adb4afb447cb99a697c0a5b95fa5",
      "6ec38c7691be44b4913d6d5e0f76603c",
      "3e20c2e2e90f4b7a8834061ddc521147",
      "bcad867ad46544d980cf600dfc4bd772",
      "dfd3a1dea41c4cc59091f8a205c9a622",
      "c5e03a3d8b16402a91bc7abbb1a32509",
      "e5b1d41caae5454a958e7341e74bd3f0",
      "c28b1cff8e334240b14dec1137dd814b",
      "d80d5f04f004404c97502df6ccc5ec89",
      "d40dbc875ca547edb0d09a07e3a84d26",
      "ae9b502b1b9f4e13ad6e7dc448b02e88",
      "2decd5779ade414d8680c5c5a48ea2ab",
      "b77b13cf1c7b4ad08c49d88a8808c2a2",
      "7de05784ffcf4f0aae1ee7e12e0fa269",
      "4b41472cec6146998f610e6d50f0c3d9",
      "04d8795812be4b838a3cbe0c3ffa6213",
      "33dde4a7859c48e1b28a972cbb66f168",
      "15912398841a42768b19ba8b000cd33d",
      "19c1ff45659147d6a6c73701cd04a457",
      "92533d2577d04fd3a606f5bbfebe997f",
      "1c6f95ef82774b918e8873427f431732",
      "079c262a15c34ee499cc7aab01e4f63d",
      "83e99b57d4884605b2cfde2c29981535",
      "8c2a09d3aec44b6db0a667d9786ec86b"
     ]
    },
    "id": "bcNEJ6perOSs",
    "outputId": "7033fc24-6f52-4c18-9e6a-6192e6727ac8"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OL3fgLvrXvH"
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODdQRnassj9s",
    "outputId": "0969f323-2865-49c1-f894-97f0de1b8071"
   },
   "outputs": [],
   "source": [
    "#y_train #converted labels into integer/numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B42CTCnrrEx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(train_encodings, y_train)\n",
    "test_dataset = SentimentDataset(test_encodings, y_test)\n",
    "\n",
    "# Wrap in DataLoader for batching/shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",     # run evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",           # save checkpoint at the end of each epoch\n",
    ")\n",
    "\n",
    "# Load model (PyTorch version)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2, #spam and ham\n",
    "    problem_type=\"single_label_classification\" #This forces CrossEntropyLoss with 2 labels\n",
    "    #num_labels=len(set(y_train))  # adjust for your dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrossEntropyLoss expects:\n",
    "#logits â†’ shape [batch_size, num_labels]\n",
    "#labels â†’ shape [batch_size] (integers like 0,1)\n",
    "#BCEWithLogitsLoss expects:\n",
    "#logits â†’ [batch_size, num_labels]\n",
    "#labels â†’ [batch_size, num_labels] (one-hot or multi-hot)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    \n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the Trainer API is moving away from the tokenizer argument. They renamed it to processing_class \n",
    "#(because it can be either a Tokenizer or a Processor, like in multimodal models).\n",
    "#trainer = Trainer(\n",
    "#    model=model,\n",
    "#    args=training_args,\n",
    "#    train_dataset=train_dataset,\n",
    "#    eval_dataset=test_dataset,\n",
    "#    tokenizer=tokenizer,   # processing_class new arg name\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inferencing\n",
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction\n",
    "y_predictions = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output labels predicted\n",
    "#trainer.predict(test_dataset)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=trainer.predict(test_dataset)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(y_test,output)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract logits\n",
    "logits = y_predictions.predictions\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Convert to predicted classes\n",
    "y_pred = np.argmax(logits, axis=-1)\n",
    "y_true = y_predictions.label_ids\n",
    "\n",
    "print(\"Predicted:\", y_pred[:10])\n",
    "print(\"True:\", y_true[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('torch_senti_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(( dict(train_encodings), y_train )) \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(( dict(test_encodings), y_test ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH1dupK0rzfn"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=2,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZvTrEcfr7k-",
    "outputId": "a47c5340-4e68-45d7-b7ce-64604fc519f8"
   },
   "outputs": [],
   "source": [
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R534aDi3xD0s",
    "outputId": "2b8eeaa4-6c2a-4b01-f2e0-c282ad5e2a3b"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvP_ktb-xyFq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UyBmI1WcxKjG",
    "outputId": "760098b0-0772-4671-8840-712d3ea89a3c"
   },
   "outputs": [],
   "source": [
    "trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Qc5FtM8xn9A",
    "outputId": "12978cf2-d6b7-4259-8dca-0453d84034fb"
   },
   "outputs": [],
   "source": [
    "trainer.predict(test_dataset)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUVX_IhWxkxg"
   },
   "outputs": [],
   "source": [
    "output=trainer.predict(test_dataset)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfCE06jQu5cI",
    "outputId": "7fd6fa95-ea5a-47fd-ea94-126f949ab8c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(y_test,output)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okD5we1NwhQW"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('tf_senti_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9MbFGrEyNTS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Custom Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
