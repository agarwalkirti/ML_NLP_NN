{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cd0983-8003-49c7-9cc4-1a15b84509ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\anaconda\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\anaconda\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\anaconda\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "# Transformers installation\n",
    "#! pip install transformers\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git\n",
    "#need to downgrade protobuf to a compatible version (e.g., 3.20.x). if running with cpu instead of gpu environment\n",
    "#!pip install protobuf==6.32.*\n",
    "#!pip install torch\n",
    "#By default, Hugging Face pipeline tries PyTorch first, and only if PyTorch isnâ€™t available does \n",
    "#it fall back to TensorFlow\n",
    "#!pip install tf-keras \n",
    "#This installs the backwards-compatible version of Keras that works with TensorFlow and Hugging Face.\n",
    "#install Jupyter widgets support:\n",
    "#!pip install ipywidgets\n",
    "#the model repo supports XetHub storage, which is faster.\n",
    "#!pip install huggingface_hub[hf_xet]\n",
    "#Install GPU version of TensorFlow\n",
    "#!pip install tensorflow[and-cuda]\n",
    "#Verify GPU is detected:\n",
    "#import tensorflow as tf\n",
    "#print(tf.config.list_physical_devices(\"GPU\"))\n",
    "#import sys\n",
    "#print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b9d5872-b954-4ddc-bada-8b93c9c5e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.0+cu121\n",
      "CUDA version: 12.1\n",
      "GPU available: True\n",
      "GPU name: NVIDIA GeForce RTX 3050 A Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch as pt\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a053c92c-adc2-4421-aa36-ea1229143235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501cbdcd-0a58-434c-8b03-73ddb3f8b916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9774676561355591}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline #using pipeline 2 things get loaded\n",
    "#pipeline helps me to call a pre trained model like bert,gpt2 etc\n",
    "#in this classifier pretrained model will get downloaded on sentiment-analysis\n",
    "#distilbert-base model used finetuned on sst-2-english -(dataset used) , uncased means lowercase dataset used\n",
    "# Make sure GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Load a small model (sentiment analysis)\n",
    "classifier1 = pipeline(\"sentiment-analysis\", device=device,model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\") \n",
    "result = classifier1(\"I finally fixed my PyTorch and Transformers setup! ðŸŽ‰\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2fec2c-3c72-418f-9acc-cfd71d7be549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8ccab8e-3ad7-4a33-b4b9-393a3f21742a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998461008071899}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1('The pizza is not that great but the crust is awesome.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67df043c-68fc-419d-8f67-5cf3a222cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier1([\"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "           \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb458ee-88f1-463e-95cd-fd1946d38d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3644b45-7e30-4c5b-b07f-1b4950082172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '3 stars', 'score': 0.33688199520111084}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Esperamos que no lo odie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2726cc9e-ee04-494c-8286-ee1c390012cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#whenever any kind of model we have to follow  below  mentioned process\n",
    "#AutoTokenizer- take test data and convert into numerical-embeddings like word2vec\n",
    "#after embeddings generation ,then only model will classify \n",
    "#TFAutoModelForSequenceClassification-if we are using tensor flow it will be used to download model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd876d7-1b21-4847-8033-ae6175ca777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "#AutoModelForSequenceClassification for pytorch\n",
    "#Use from_pt=True only if you want to convert a PyTorch model to TensorFlow.If youâ€™re running in PyTorch only, just omit the flag.\n",
    "#device=0 ensures the pipeline runs on GPU.\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Use pipeline on GPU if available\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6c9525-5059-488f-8cb7-74ed88c661c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '1 star', 'score': 0.7558020353317261}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am a bad boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "047621a5-1fb2-4d33-a939-67f70741c0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '4 stars', 'score': 0.42292681336402893}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I am a good boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a38ae-35bb-40ee-9a35-12df28279bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference on a batch (no pipeline)\n",
    "#understand exactly what happens: tokenize â†’ move to device(gpu) â†’ forward pass â†’ softmax â†’ labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20213922-9f76-4a8e-97e3-f4487de93a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5a599048e449d4863db59629b4e070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7af844459ee4349a6f0a16ee398f74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc728876-6a3d-4041-9b9c-02836c53c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b865fd-6b7c-49c8-b9eb-ac64a48d86ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#every word is given a numerical number-indexes works as id to words\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9c3ff0b-2293-46f5-b3a5-a65a5b160fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding to make all sentences of equal length of 512\n",
    "#pt_batch is a BatchEncoding (basically a dict with input_ids, attention_mask, etc.).\n",
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\" #\"pt\"-- use \"pt\" for PyTorch tensors , \"tf\"-tensorflow tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "953153b4-69f5-4ea4-9781-44a4c13ed04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Show tokenized tensors\n",
    "for key, value in pt_batch.items():\n",
    "    print(f\"{key}: {value.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d242770-e142-4272-ba93-e9db0dcf7a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels must be torch.LongTensor (class indices 0..4 for this model)\n",
    "labels = torch.tensor([1, 0], dtype=torch.long)\n",
    "#torch.tensor([1, 0], dtype=torch.long) â†’ creates a tensor of class indices.\n",
    "#(e.g., 1 = 2 stars, 0 = 1 star for this model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37662b73-8e99-4974-a90f-e4203fb684a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In PyTorch, Hugging Face models donâ€™t accept a whole BatchEncoding object directly.\n",
    "#we need to unpack the dict into keyword arguments.\n",
    "# not this pt_outputs = pt_model(pt_batch)\n",
    "# Forward pass (unpack with **)\n",
    "# unpack the dict and pass labels\n",
    "pt_outputs = pt_model(**pt_batch, labels=labels)\n",
    "#The model expects arguments like model(input_ids=..., attention_mask=...).\n",
    "#Using **pt_batch unpacks the dict into those arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7297f4e-57f2-4fa8-b6f2-7209ccfdb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.3167, grad_fn=<NllLossBackward0>), logits=tensor([[-4.0833,  4.3364],\n",
      "        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "#pt_model(**pt_batch, labels=labels) â†’ automatically computes cross-entropy loss + returns logits.\n",
    "#pt_outputs.loss â†’ scalar loss (use in training).\n",
    "#pt_outputs.logits â†’ raw predictions per class\n",
    "print(pt_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57804d82-3f42-4333-a51e-d7ba0f9a36c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3167364001274109\n",
      "Logits: tensor([[-4.0833,  4.3364],\n",
      "        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\", pt_outputs.loss.item())\n",
    "print(\"Logits:\", pt_outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef6cef05-83a9-45aa-9713-68e52abad90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf #for tensorflow\n",
    "#tf_predictions = tf.nn.softmax(pt_outputs[0], axis=-1)\n",
    "import torch.nn.functional as ActivationFunc\n",
    "# Apply softmax on logits to get probabilities\n",
    "pt_predictions = ActivationFunc.softmax(pt_outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8415a278-0f92-414a-b5f8-8bb610ff6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2043e-04, 9.9978e-01],\n",
      "        [5.3086e-01, 4.6914e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03a02b16-87c8-4a8c-ac93-c954abefc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"./saved_model\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bb1b75e-5412-43e2-bca0-1b565c41f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d6f22bc-035f-4145-9d6c-7e3fef4e6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_outputs = pt_model(**pt_batch, output_hidden_states=True)\n",
    "all_hidden_states, all_attentions = pt_outputs[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18b0d076-5f0a-4eef-8999-770cf136261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4da35fe1-7e6e-4a4c-a0ad-52850b0910ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ebfd4-ef22-452e-a4da-a8b85f06e120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2613770-5e04-4726-9502-f3360772fc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1d3d7-74d8-481d-a306-1d5a95904255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f2511-c99d-44e1-b043-4fe532c52191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ee5a2-109d-4705-a1b5-6714161981dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
